
# Helper functions ----
# Function to find the max page number
get_max_page <- function(url) {
  page <- read_html(url)
  
  # Extract pagination numbers
  pagination_numbers <- page %>%
    html_nodes(".styled__DesktopPaginationControls-sc-4zz1jl-1 .styled__ButtonWrapper-sc-dscagy-1 span") %>%
    html_text(trim = TRUE) %>%
    as.numeric()
  
  # Find the maximum number, ignoring any non-numeric entries (e.g., '...') and NA values
  max_page <- max(pagination_numbers, na.rm = TRUE)
  
  return(max_page)
}


# 0) Setup ----
library(tidyverse)
library(rvest)
library(janitor)
library(stringr)
library(readr)
library(lubridate)
library(forcats)
library(scales)

Sys.setenv(lang = "en_US")
options(scipen = 999)
theme_set(new = theme_light())

# 1) Load data (web scraping) ----
# Initialize empty df to bind_rows() the different search results
search_df <- data.frame(
  Ranking = NA,
  Community = NA,
  Meta = NA,
  Search = NA,
  stringsAsFactors = FALSE
) 



# Define the base URL
base_url <- "https://www.skool.com/discovery"
search_term <- "academy" # ACADEMY, Academia, etc.

# search_terms <- c("academy", "community")

#search_term <- c("academy")

# saveRDS(object = temp_data, 
#         file = paste0("E:/The Data Digest/GitHub/Skool/input/search_community_1_", search_term, today(), ".RDS"))
#academy <- readRDS("E:/The Data Digest/GitHub/Skool/input/search_community_1_academy2024-08-14.RDS")

search_df <- bind_rows(search_df, temp_data)

dim(search_df)
saveRDS(object = search_df, 
        file = paste0("E:/The Data Digest/GitHub/Skool/input/search_community_44_", today(), ".RDS"))
#search_df <- readRDS(file = "E:/The Data Digest/GitHub/Skool/input/search_community_10_2024-08-14.RDS")

(search_term <- search_terms_community[44])
# # Start search term loop
# for(term in search_terms){
#     
#     temp_search_term <- term
#     
    # Initialize empty lists to store the data
    rankings <- list()
    community_names <- list()
    meta_info <- list()

    # build url
    search_url <- paste0("https://www.skool.com/discovery?q=", search_term)
      
    # extract last page number
    max_page <- get_max_page(url = search_url)
      
    # Get max page number
    print(search_term)
    print(max_page)
    
    # Loop through pages up to the max page
    for(page_num in 1:max_page){
      
  #     # time control, simulate manual clicking
  # time <- round(sample(x = 2:10, size = 1) + rnorm(1), 2)
  # date_time <- Sys.time()
  # while((as.numeric(Sys.time()) - as.numeric(date_time)) < time) {
  
      
      # page_url
      page_url <- paste0(search_url, "&p=", page_num) 
  
      # Read the HTML content of the webpage
      temp_page <- read_html(page_url)
      
      # Extract the community name
      community_names[[page_num]] <- temp_page %>%
        html_nodes('.styled__TypographyWrapper-sc-m28jfn-0.eoHmvk') %>%
        html_text()
      
      # Extract the ranking
      rankings[[page_num]] <- temp_page %>%
        html_nodes('.styled__DiscoveryCardRanking-sc-13ysp3k-6.egeJZg') %>%
        html_text()
      
      # Extract the meta information (Private/Public, Members, Paid/Free)
      meta_info[[page_num]] <- temp_page %>%
        html_nodes('.styled__DiscoveryCardMeta-sc-13ysp3k-7.jjNZwk') %>%
        html_text()
  
 }
  
  # Combine the lists into a single data frame
  temp_data <- data.frame(
    Ranking = unlist(rankings),
    Community = unlist(community_names),
    Meta = unlist(meta_info),
    stringsAsFactors = FALSE
  ) %>% mutate(Search = search_term)

search_df <- bind_rows(search_df, temp_data)

# Display the extracted data
dim(search_df)

#str(search_df)

# analyze the % name hit in title. Guild almost 100%, group almost 0%
# important to being search in discovery

# table, name, pages/#communities (competition), % title match, real competition

# make scatterplot (ggtext) and line, high vs low competition terms
# has to feel authentic

# talk about search terms, sometimes appearing in description ("minds", "movement")
# but not really clear why certain communities appear for certain words
# Maybe Sam Ovens can tell us

# Analzye search terms by free/paid/paid_USD % stacked bar chart (horizontal)
# same for public vs private (ordered by category free/private etc.)

# log transformed histogram of member numbers? many low categories, 
# name and price (log10 box plot) is going to be most interesting

# reactable html widget
# ! red bar based on members x price estimation of group value

# check if color code for price makes sense high mid low, is it gradient or only 5 values? Yes looks like it!


# 2) Data cleaning and manipulation ----
skool_df <- search_df
skool_df <- clean_names(skool_df)

skool_df <- skool_df %>% mutate(
  id = parse_number(ranking),
  comm_emoji = str_detect(community, "[^\x20-\x7E]"),
  desc_emoji = str_detect(description, "[^\x20-\x7E]"),
)

# Split the "meta" column into "privacy", "members", and "access"
skool_df <- skool_df %>%
  separate(meta, into = c("privacy", "members", "access"), sep = "  â€¢  ", remove = FALSE)


# Create a new column "Numeric_Members" from "Members"
skool_df <- skool_df %>%
  mutate(
    members_cleaned = str_remove(members, " Members"),
    members_n = case_when(
      str_detect(members_cleaned, "k") ~ as.numeric(str_remove(members_cleaned, "k")) * 1000,
      TRUE ~ as.numeric(members_cleaned)),
    price = parse_number(access))

skool_df$access2 <- skool_df$access
skool_df$access2[!is.na(skool_df$price)] <- "Paid_USD"

View(skool_df)

# check search term being part of title
skool_df$community <- tolower(skool_df$community)

skool_df$term_in_title <- str_detect(string = skool_df$community, pattern = skool_df$search)
View(search_df)

skool_df %>% group_by(search) %>% summarize(mean(term_in_title), n())
# already a fun and interesting result
# SEO use business?

###! RANK frequencies look really strange? ----
# Why number #1 68 times? could not be more common than search terms
# p was missing in "&p=", page_url <- paste0(search_url, "&p=", page_num) 

## add date to data-frame
today()

# 3) Save objects ----

# save.image(file = "euro2024.RData")
# Save an object to a file
skool1000 <- skool_df
saveRDS(object = skool1000, 
        file = paste0("E:/The Data Digest/GitHub/Skool/input/skool1000_", today(), ".RDS"))



# 4) Search terms backup ----

search_terms_community <- c("academy", "agency", "bootcamp", "brand", "brotherhood", "business", "circle", "club", "collective", "community", "crew", "dojo", "elite","empire", "family", "forum", "gang", "generation", "group", "guild", "herd", "home","kingdom", "klub", "lab", "league", "legion", "masterclass", "mastermind", "meets", "movement","nation", "preschool", "school", "skool", "society", "squad","startup", "studio", "system", "team", "tribe", "university", "workshop")

search_terms_individual_title <- c("achiever", "anonymous", "apprentice", "architects", "artist", "builder", "coach", "creatives", "creator", "developer", "editor", "engineer", "entrepreneurs", "expert", "founder", "friends", "girls", "humans", "husband", "immigrant", "influencer", "insider", "kevin", "king", "kyle", "leader",  "lions", "lords", "major", "marketer", "master", "member", "mentor",  "men", "millionaire", "mogul", "muslim",  "nerd", "ninjas", "nomad", "noob", "one", "operator", "outlier", "owners", "parents", "partners", "pathfinders",  "photographers", "pilots", "player", "practitioner", "producer", "prodigies", "professionals", "pursuers", "queen", "raiser", "rebels", "resellpreneur", "rick", "royalty", "sage", "saint",   "self", "sellers", "servant", "servants", "setters", "shepherds", "shesells", "sharks", "shredders", "singers", "sisyphus","sisterhood",  "sisterheard", "sniper", "socializer", "solo", "star", "starter", "students", "superhuman", "superior", "supermums", "surfer", "synthesizer", "tailors", "talent", "tarzans", "taylor", "teacher", "tester", "tom", "trader", "trainer", "tycoon", "unicorn", "users", "vegans", "vip", "warrior", "winner", "woman", "women", "worker")

search_terms_others <- c("accelerator", "access", "acquisition", "action", "basecamp", "best", "challenge", "channels", "coaching", "day", "digital", "education", "exclusive", "fit", "free", "game", "grow", "habit", "healing", "health", "help", "hustle", "impact", "income", "instagram", "international", "internet", "invest", "land", "launch", "learn", "legacy", "life", "magnet", "market", "media", "mission", "money", "motivation", "official", "onlyfans", "passion", "power", "premium", "product", "profit", "project", "public", "purpose", "radikal", "reiki", "remote", "renaissance", "result", "retire", "revolution", "rich", "sales", "secret", "service", "shopify", "skills", "social", "solution", "spiritually", "sports", "strategy", "strength", "study", "success", "support", "tarot", "teach", "thailand", "therapy", "thrive", "ticket", "time", "tools", "top", "track", "trading", "training", "ultimate", "universe", "unlimited", "value", "wealth", "watercolor", "weekend", "website", "white", "world", "yacht", "youtube")


search_terms_community_BU <- c("academy", "agencies", "agency", "alliance", "army", "beehive", "bootcamp", "brand", "brotherhood", "business","camp", "circle", "club", "collective", "college", "community", "corp", "crew", "dojo", "elite","empire", "exclusive", "family", "forum", "gang", "generation", "group", "guild", "herd", "home","kingdom", "klub", "lab", "league", "legion", "masterclass", "mastermind", "meets", "minds",  "movement","nation", "ninjas", "preschool", "school", "sisterhood", "skool", "sobriety", "society", "space", "squad","startup", "studio", "system", "team", "together", "tribe", "university", "workshop")

# Handle non-existing pages
tryCatch({
  page_data <- read_html(page_url)
  # Do your scraping here
}, error = function(e) {
  message(paste("Page", i, "does not exist. Stopping loop."))
  break
})


