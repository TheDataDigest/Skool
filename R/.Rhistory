ls()
# Helper functions ----
# Function to find the max page number
get_max_page <- function(url) {
page <- read_html(url)
# Extract pagination numbers
pagination_numbers <- page %>%
html_nodes(".styled__DesktopPaginationControls-sc-4zz1jl-1 .styled__ButtonWrapper-sc-dscagy-1 span") %>%
html_text(trim = TRUE) %>%
as.numeric()
# Find the maximum number, ignoring any non-numeric entries (e.g., '...') and NA values
max_page <- max(pagination_numbers, na.rm = TRUE)
return(max_page)
}
# 0) Setup ----
library(tidyverse)
library(rvest)
library(janitor)
library(stringr)
library(readr)
library(lubridate)
library(forcats)
library(scales)
Sys.setenv(lang = "en_US")
options(scipen = 999)
theme_set(new = theme_light())
# Define the base URL
base_url <- "https://www.skool.com/discovery"
search_term <- "academy" # ACADEMY, Academia, etc.
search_terms_community2
search_terms_community2 <- c("camp", "circle", "club", "collective", "college", "community", "corp", "crew", "dojo", "elite")
ls()
# Initialize empty df to bind_rows() the different search results
search_df <- data.frame(
Ranking = NA,
Community = NA,
Description = NA,
Meta = NA,
Background_Image_URL = NA,
Search = NA,
stringsAsFactors = FALSE
)
# Start search term loop
for(term in search_terms){
temp_search_term <- term
# Initialize empty lists to store the data
rankings <- list()
community_names <- list()
descriptions <- list()
meta_info <- list()
background_image_urls <- list()
# build url
search_url <- paste0("https://www.skool.com/discovery?q=", temp_search_term)
# extract last page number
max_page <- get_max_page(url = search_url)
# Get max page number
print(temp_search_term)
print(max_page)
# Loop through pages up to the max page
for(page_num in 1:max_page){
# page_url
page_url <- paste0(search_url, "&=", page_num)
# Read the HTML content of the webpage
temp_page <- read_html(url)
# Extract the community name
community_names[[page_num]] <- temp_page %>%
html_nodes('.styled__TypographyWrapper-sc-m28jfn-0.eoHmvk') %>%
html_text()
# Extract the ranking
rankings[[page_num]] <- temp_page %>%
html_nodes('.styled__DiscoveryCardRanking-sc-13ysp3k-6.egeJZg') %>%
html_text()
# Extract the description
descriptions[[page_num]] <- temp_page %>%
html_nodes('.styled__DiscoveryCardDescription-sc-13ysp3k-5.dCJqtG') %>%
html_text()
# Extract the meta information (Private/Public, Members, Paid/Free)
meta_info[[page_num]] <- temp_page %>%
html_nodes('.styled__DiscoveryCardMeta-sc-13ysp3k-7.jjNZwk') %>%
html_text()
# Extract the image URLs and handle missing values
image_elements <- temp_page %>%
html_nodes('.styled__AvatarWrapper-sc-140v536-0.hDDweh img, .styled__AvatarWrapper-sc-140v536-0.hDDweh')
# Extract the background image URLs and handle missing values
bg_image_elements <- temp_page %>%
html_nodes('.styled__DiscoveryCardCover-sc-13ysp3k-2.hSDuvz, .styled__DiscoveryCardCoverPlaceholder-sc-13ysp3k-3.kXNGWM')
background_image_urls[[page_num]] <- sapply(bg_image_elements, function(node) {
style_attr <- html_attr(node, 'style')
if (!is.null(style_attr)) {
url <- str_extract(style_attr, 'url\\((.*?)\\)')
if (!is.na(url)) {
str_replace_all(url, 'url\\(|\\)|"|&quot;', '')
} else {
NA
}
} else {
NA
}
})
}
# Combine the lists into a single data frame
temp_data <- data.frame(
Ranking = unlist(rankings),
Community = unlist(community_names),
Description = unlist(descriptions),
Meta = unlist(meta_info),
Background_Image_URL = unlist(background_image_urls),
stringsAsFactors = FALSE
) %>% mutate(Search = temp_search_term)
search_df <- bind_rows(search_df, temp_data)
}
search_terms_community1 <- c("academy", "agencies", "agency", "alliance", "army", "beehive", "bootcamp", "brand", "brotherhood", "business")
search_terms_community2 <- c("camp", "circle", "club", "collective", "college", "community", "corp", "crew", "dojo", "elite")
search_terms_community3 <- c("empire", "exclusive", "family", "forum", "gang", "generation", "group", "guild", "herd", "home")
search_terms_community4 <- c("kingdom", "klub", "lab", "league", "legion", "masterclass", "mastermind", "meets", "minds",  "movement")
search_terms_community5 <- c("nation", "ninjas", "preschool", "school", "sisterhood", "skool", "sobriety", "society", "space", "squad")
search_terms_community6 <- c("startup", "studio", "system", "team", "together", "tribe", "university", "workshop")
search_terms <- search_terms_community1 # 1-6
# Initialize empty df to bind_rows() the different search results
search_df <- data.frame(
Ranking = NA,
Community = NA,
Description = NA,
Meta = NA,
Background_Image_URL = NA,
Search = NA,
stringsAsFactors = FALSE
)
# Start search term loop
for(term in search_terms){
temp_search_term <- term
# Initialize empty lists to store the data
rankings <- list()
community_names <- list()
descriptions <- list()
meta_info <- list()
background_image_urls <- list()
# build url
search_url <- paste0("https://www.skool.com/discovery?q=", temp_search_term)
# extract last page number
max_page <- get_max_page(url = search_url)
# Get max page number
print(temp_search_term)
print(max_page)
# Loop through pages up to the max page
for(page_num in 1:max_page){
# page_url
page_url <- paste0(search_url, "&=", page_num)
# Read the HTML content of the webpage
temp_page <- read_html(url)
# Extract the community name
community_names[[page_num]] <- temp_page %>%
html_nodes('.styled__TypographyWrapper-sc-m28jfn-0.eoHmvk') %>%
html_text()
# Extract the ranking
rankings[[page_num]] <- temp_page %>%
html_nodes('.styled__DiscoveryCardRanking-sc-13ysp3k-6.egeJZg') %>%
html_text()
# Extract the description
descriptions[[page_num]] <- temp_page %>%
html_nodes('.styled__DiscoveryCardDescription-sc-13ysp3k-5.dCJqtG') %>%
html_text()
# Extract the meta information (Private/Public, Members, Paid/Free)
meta_info[[page_num]] <- temp_page %>%
html_nodes('.styled__DiscoveryCardMeta-sc-13ysp3k-7.jjNZwk') %>%
html_text()
# Extract the image URLs and handle missing values
image_elements <- temp_page %>%
html_nodes('.styled__AvatarWrapper-sc-140v536-0.hDDweh img, .styled__AvatarWrapper-sc-140v536-0.hDDweh')
# Extract the background image URLs and handle missing values
bg_image_elements <- temp_page %>%
html_nodes('.styled__DiscoveryCardCover-sc-13ysp3k-2.hSDuvz, .styled__DiscoveryCardCoverPlaceholder-sc-13ysp3k-3.kXNGWM')
background_image_urls[[page_num]] <- sapply(bg_image_elements, function(node) {
style_attr <- html_attr(node, 'style')
if (!is.null(style_attr)) {
url <- str_extract(style_attr, 'url\\((.*?)\\)')
if (!is.na(url)) {
str_replace_all(url, 'url\\(|\\)|"|&quot;', '')
} else {
NA
}
} else {
NA
}
})
}
# Combine the lists into a single data frame
temp_data <- data.frame(
Ranking = unlist(rankings),
Community = unlist(community_names),
Description = unlist(descriptions),
Meta = unlist(meta_info),
Background_Image_URL = unlist(background_image_urls),
stringsAsFactors = FALSE
) %>% mutate(Search = temp_search_term)
search_df <- bind_rows(search_df, temp_data)
}
search_terms
search_df
search_df
temp_search_term
term
rankings
search_url
page_url
# Read the HTML content of the webpage
temp_page <- read_html(url)
library(rvest)
# Start search term loop
for(term in search_terms){
temp_search_term <- term
# Initialize empty lists to store the data
rankings <- list()
community_names <- list()
descriptions <- list()
meta_info <- list()
background_image_urls <- list()
# build url
search_url <- paste0("https://www.skool.com/discovery?q=", temp_search_term)
# extract last page number
max_page <- get_max_page(url = search_url)
# Get max page number
print(temp_search_term)
print(max_page)
# Loop through pages up to the max page
for(page_num in 1:max_page){
# page_url
page_url <- paste0(search_url, "&=", page_num)
# Read the HTML content of the webpage
temp_page <- read_html(page_url)
# Extract the community name
community_names[[page_num]] <- temp_page %>%
html_nodes('.styled__TypographyWrapper-sc-m28jfn-0.eoHmvk') %>%
html_text()
# Extract the ranking
rankings[[page_num]] <- temp_page %>%
html_nodes('.styled__DiscoveryCardRanking-sc-13ysp3k-6.egeJZg') %>%
html_text()
# Extract the description
descriptions[[page_num]] <- temp_page %>%
html_nodes('.styled__DiscoveryCardDescription-sc-13ysp3k-5.dCJqtG') %>%
html_text()
# Extract the meta information (Private/Public, Members, Paid/Free)
meta_info[[page_num]] <- temp_page %>%
html_nodes('.styled__DiscoveryCardMeta-sc-13ysp3k-7.jjNZwk') %>%
html_text()
# Extract the image URLs and handle missing values
image_elements <- temp_page %>%
html_nodes('.styled__AvatarWrapper-sc-140v536-0.hDDweh img, .styled__AvatarWrapper-sc-140v536-0.hDDweh')
# Extract the background image URLs and handle missing values
bg_image_elements <- temp_page %>%
html_nodes('.styled__DiscoveryCardCover-sc-13ysp3k-2.hSDuvz, .styled__DiscoveryCardCoverPlaceholder-sc-13ysp3k-3.kXNGWM')
background_image_urls[[page_num]] <- sapply(bg_image_elements, function(node) {
style_attr <- html_attr(node, 'style')
if (!is.null(style_attr)) {
url <- str_extract(style_attr, 'url\\((.*?)\\)')
if (!is.na(url)) {
str_replace_all(url, 'url\\(|\\)|"|&quot;', '')
} else {
NA
}
} else {
NA
}
})
}
# Combine the lists into a single data frame
temp_data <- data.frame(
Ranking = unlist(rankings),
Community = unlist(community_names),
Description = unlist(descriptions),
Meta = unlist(meta_info),
Background_Image_URL = unlist(background_image_urls),
stringsAsFactors = FALSE
) %>% mutate(Search = temp_search_term)
search_df <- bind_rows(search_df, temp_data)
}
search1 <- search_df
saveRDS(object = search1,
file = paste0("E:/The Data Digest/GitHub/Skool/input/search1_", today(), ".RDS"))
search_df
names(search_df)
str_detect(string = search_df$Community, pattern = search_df$Search)
table(.Last.value)
table(search_df$Search)
View(search_df)
search_df$Community
tolower(search_df$Community)
# check search term being part of title
search_df$Community <- tolower(search_df$Community)
str_detect(string = search_df$Community, pattern = search_df$Search)
table(.Last.value)
search_df$term_in_title <- str_detect(string = search_df$Community, pattern = search_df$Search)
View(search_df)
search_df %>% group_by(Search) %>% summarize(mean(term_in_title))
search_df %>% group_by(Search) %>% summarize(mean(term_in_title), n())
parsenumber(search_df$Ranking)
?parse_number
parse_number(search_df$Ranking)
max(parse_number(search_df$Ranking))
max(parse_number(search_df$Ranking), na.rm = T)
table(parse_number(search_df$Ranking))
table(table(parse_number(search_df$Ranking)))
as.data.frame(table(parse_number(search_df$Ranking)))
as.data.frame(table((search_df$Ranking)))
page_num
page_url
search_terms_community1
search_terms <- search_terms_community1 # 1-6
# Initialize empty df to bind_rows() the different search results
search_df <- data.frame(
Ranking = NA,
Community = NA,
Description = NA,
Meta = NA,
Background_Image_URL = NA,
Search = NA,
stringsAsFactors = FALSE
)
search_terms
search_url
page_url
paste0(search_url, "&p=", page_num)
# Start search term loop
for(term in search_terms){
temp_search_term <- term
# Initialize empty lists to store the data
rankings <- list()
community_names <- list()
descriptions <- list()
meta_info <- list()
background_image_urls <- list()
# build url
search_url <- paste0("https://www.skool.com/discovery?q=", temp_search_term)
# extract last page number
max_page <- get_max_page(url = search_url)
# Get max page number
print(temp_search_term)
print(max_page)
# Loop through pages up to the max page
for(page_num in 1:max_page){
# page_url
page_url <- paste0(search_url, "&p=", page_num)
# Read the HTML content of the webpage
temp_page <- read_html(page_url)
# Extract the community name
community_names[[page_num]] <- temp_page %>%
html_nodes('.styled__TypographyWrapper-sc-m28jfn-0.eoHmvk') %>%
html_text()
# Extract the ranking
rankings[[page_num]] <- temp_page %>%
html_nodes('.styled__DiscoveryCardRanking-sc-13ysp3k-6.egeJZg') %>%
html_text()
# Extract the description
descriptions[[page_num]] <- temp_page %>%
html_nodes('.styled__DiscoveryCardDescription-sc-13ysp3k-5.dCJqtG') %>%
html_text()
# Extract the meta information (Private/Public, Members, Paid/Free)
meta_info[[page_num]] <- temp_page %>%
html_nodes('.styled__DiscoveryCardMeta-sc-13ysp3k-7.jjNZwk') %>%
html_text()
# Extract the image URLs and handle missing values
image_elements <- temp_page %>%
html_nodes('.styled__AvatarWrapper-sc-140v536-0.hDDweh img, .styled__AvatarWrapper-sc-140v536-0.hDDweh')
# Extract the background image URLs and handle missing values
bg_image_elements <- temp_page %>%
html_nodes('.styled__DiscoveryCardCover-sc-13ysp3k-2.hSDuvz, .styled__DiscoveryCardCoverPlaceholder-sc-13ysp3k-3.kXNGWM')
background_image_urls[[page_num]] <- sapply(bg_image_elements, function(node) {
style_attr <- html_attr(node, 'style')
if (!is.null(style_attr)) {
url <- str_extract(style_attr, 'url\\((.*?)\\)')
if (!is.na(url)) {
str_replace_all(url, 'url\\(|\\)|"|&quot;', '')
} else {
NA
}
} else {
NA
}
})
}
# Combine the lists into a single data frame
temp_data <- data.frame(
Ranking = unlist(rankings),
Community = unlist(community_names),
Description = unlist(descriptions),
Meta = unlist(meta_info),
Background_Image_URL = unlist(background_image_urls),
stringsAsFactors = FALSE
) %>% mutate(Search = temp_search_term)
search_df <- bind_rows(search_df, temp_data)
}
search_df
search_df$Ranking %>% table()
as.data.frame(search_df$Ranking %>% table())
View(search_df)
# 2) Data cleaning and manipulation ----
skool_df <- search_df
skool_df <- clean_names(data)
skool_df <- skool_df %>% mutate(
id = parse_number(ranking),
comm_emoji = str_detect(community, "[^\x20-\x7E]"),
desc_emoji = str_detect(description, "[^\x20-\x7E]"),
)
skool_df <- clean_names(skool_df)
library(janitor)
skool_df
skool_df <- clean_names(skool_df)
skool_df <- skool_df %>% mutate(
id = parse_number(ranking),
comm_emoji = str_detect(community, "[^\x20-\x7E]"),
desc_emoji = str_detect(description, "[^\x20-\x7E]"),
)
# Split the "meta" column into "privacy", "members", and "access"
skool_df <- skool_df %>%
separate(meta, into = c("privacy", "members", "access"), sep = "  â€¢  ", remove = FALSE)
# Create a new column "Numeric_Members" from "Members"
skool_df <- skool_df %>%
mutate(
members_cleaned = str_remove(members, " Members"),
members_n = case_when(
str_detect(members_cleaned, "k") ~ as.numeric(str_remove(members_cleaned, "k")) * 1000,
TRUE ~ as.numeric(members_cleaned)),
price = parse_number(access))
skool_df$access2 <- skool_df$access
skool_df$access2[!is.na(skool_df$price)] <- "Paid_USD"
# check search term being part of title
skool_df$community <- tolower(skool_df$community)
skool_df$term_in_title <- str_detect(string = skool_df$community, pattern = skool_df$search)
View(search_df)
skool_df %>% group_by(search) %>% summarize(mean(term_in_title), n())
search_term
term
search_url
max_page
# Loop through pages up to the max page
for(page_num in 1:max_page){
# page_url
page_url <- paste0(search_url, "&p=", page_num)
# Read the HTML content of the webpage
temp_page <- read_html(page_url)
# Extract the community name
community_names[[page_num]] <- temp_page %>%
html_nodes('.styled__TypographyWrapper-sc-m28jfn-0.eoHmvk') %>%
html_text()
# Extract the ranking
rankings[[page_num]] <- temp_page %>%
html_nodes('.styled__DiscoveryCardRanking-sc-13ysp3k-6.egeJZg') %>%
html_text()
# Extract the description
descriptions[[page_num]] <- temp_page %>%
html_nodes('.styled__DiscoveryCardDescription-sc-13ysp3k-5.dCJqtG') %>%
html_text()
# Extract the meta information (Private/Public, Members, Paid/Free)
meta_info[[page_num]] <- temp_page %>%
html_nodes('.styled__DiscoveryCardMeta-sc-13ysp3k-7.jjNZwk') %>%
html_text()
# Extract the image URLs and handle missing values
image_elements <- temp_page %>%
html_nodes('.styled__AvatarWrapper-sc-140v536-0.hDDweh img, .styled__AvatarWrapper-sc-140v536-0.hDDweh')
# Extract the background image URLs and handle missing values
bg_image_elements <- temp_page %>%
html_nodes('.styled__DiscoveryCardCover-sc-13ysp3k-2.hSDuvz, .styled__DiscoveryCardCoverPlaceholder-sc-13ysp3k-3.kXNGWM')
background_image_urls[[page_num]] <- sapply(bg_image_elements, function(node) {
style_attr <- html_attr(node, 'style')
if (!is.null(style_attr)) {
url <- str_extract(style_attr, 'url\\((.*?)\\)')
if (!is.na(url)) {
str_replace_all(url, 'url\\(|\\)|"|&quot;', '')
} else {
NA
}
} else {
NA
}
})
}
skool_df %>% group_by(search) %>% summarize(mean(term_in_title), n())
114*0.36
1000*0.089
